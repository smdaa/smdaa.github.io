<!DOCTYPE html>
<html lang="en-us">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Building an autograd library from scratch in C for simple neural networks | smdaa</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    
  </head>

  <body>
    <nav>
    <ul class="menu">
      
      <li><a href="/">Home</a></li>
      
      <li><a href="/post/">Posts</a></li>
      
      <li><a href="/tags/">Tags</a></li>
      
    </ul>
    <hr/>
    </nav>

<div class="article-meta">
<h1><span class="title">Building an autograd library from scratch in C for simple neural networks</span></h1>

<h2 class="date">2024/07/29</h2>
</div>

<main>
<div class="toc">
  <h2>Table of Contents</h2>
  <nav id="TableOfContents">
  <ul>
    <li><a href="#introduction">Introduction</a></li>
    <li><a href="#neural-networks-a-brief-overview">Neural networks: a brief overview</a></li>
    <li><a href="#derivative-calculation-symbolic-numerical-and-automatic-differentiation">Derivative calculation: symbolic, numerical and automatic differentiation</a></li>
    <li><a href="#implementation">Implementation</a>
      <ul>
        <li><a href="#n-dimensional-array">N-dimensional array</a></li>
        <li><a href="#variable-node">Variable node</a></li>
        <li><a href="#multilayer-perceptron">Multilayer perceptron</a></li>
      </ul>
    </li>
    <li><a href="#tests">Tests</a></li>
    <li><a href="#examples">Examples</a>
      <ul>
        <li><a href="#mnsit">MNSIT</a></li>
        <li><a href="#paint">Paint</a></li>
      </ul>
    </li>
    <li><a href="#conclusion">Conclusion</a></li>
  </ul>
</nav>
</div>
<h2 id="introduction">Introduction</h2>
<p>Autograd is a fundamental component in machine learning frameworks, enabling the automatic computation of gradients for training neural networks. This article will walk you through my journey of writing an Autograd library from scratch (no third-party libraries) in pure C.</p>
<p><a href="https://github.com/smdaa/teeny-autograd-c">View source code on GitHub</a></p>
<h2 id="neural-networks-a-brief-overview">Neural networks: a brief overview</h2>
<p>At its core, a neural network consists of neurons organized in layers. Each neuron receives input from the previous layer, processes it using a weighted sum, applies an activation function, and passes the output to the next layer.</p>
<p><img src="/assets/building-an-autograd-library-from-scratch-in-c-for-simple-neural-networks/neuron.png" alt=""></p>
<p>Mathematically, we can express the output of a single neuron as:</p>
<p>$$
y = f(\sum_{i=1}^{n} w_i x_i + b)
$$</p>
<p>where $x_i$​ are the inputs, $w_i$​​ are the weights, $b$ is the bias, and $f$ is the activation function.</p>
<p>A layer is simply a collection of neurons, and neural networks typically consist of three types of layers: input, hidden, and output layers.</p>
<p><img src="/assets/building-an-autograd-library-from-scratch-in-c-for-simple-neural-networks/neuron-network.png" alt=""></p>
<p>Neural networks learn by adjusting the weights $w_i$ and bias $b$ of each neuron to minimize the error in their predictions. This is done via <a href="https://en.wikipedia.org/wiki/Gradient_descent">gradient descent</a>, where the network computes the gradient of the error for each weight and bias and then updates them in the direction that reduces the error.</p>
<h2 id="derivative-calculation-symbolic-numerical-and-automatic-differentiation">Derivative calculation: symbolic, numerical and automatic differentiation</h2>
<p>There are three fundamental ways to calculate derivatives:</p>
<ul>
<li>
<p><strong>Symbolic differentiation</strong>: It involves finding the exact derivative of a function using algebraic rules. If the function has a known mathematical expression, we can compute its derivative symbolically. For example for $f(x) = x^2$ the derivative is $f&rsquo;(x) = 2x$. This method can lead to unwieldy expressions, especially for functions involving products, quotients, or compositions of multiple functions.</p>
</li>
<li>
<p><strong>Numerical differentiation</strong>: These methods estimate the derivative by using values of the function at specific points. A common finite difference formula for the first derivative is given by $f&rsquo;(x) = \frac{f(x+h) - f(x)}{h}$, $h$ is a small step size. However, finite difference methods are not well-suited for neural networks, mainly for efficiency reasons: neural networks typically involve a large number of parameters (weights and biases). Calculating the gradient for each parameter using finite differences would require evaluating the function multiple times, leading to a significant computational overhead.</p>
</li>
<li>
<p><strong>Automatic differentiation</strong>: Automatic differentiation works by breaking down a function into basic mathematical components and creating a graph. In this graph, the nodes represent variables and operations, while the edges connect each operation to its input variables.</p>
</li>
</ul>
<p>Let&rsquo;s review a straightforward example of automatic differentiation to make things clearer. Let $f$ be the function $f(x, y) = x^2 +xy +2$, we will try to find the partial derivatives of $f$ with automatic differentiation.</p>
<p>First let&rsquo;s represent the function $f$ as a graph:</p>
<p><img src="/assets/building-an-autograd-library-from-scratch-in-c-for-simple-neural-networks/autograd-basic.png" alt=""></p>
<p>Now that we have broken the function $f$ to the two mathematical operations $*$ and $+$ which we know how to differentiate, let&rsquo;s add the local derivatives to the graph.</p>
<p><img src="/assets/building-an-autograd-library-from-scratch-in-c-for-simple-neural-networks/autograd-basic-reverse.png" alt=""></p>
<p>To find the partial derivatives of $f$ we need to find the path from $d$ to $x$ and apply the chain rule. This involves computing the partial derivatives of intermediate variables along this path:</p>
<p>$$
\frac{\partial d}{\partial x} = \frac{\partial d}{\partial c} . \frac{\partial c}{\partial x} = \frac{\partial d}{\partial c} . (\frac{\partial c}{\partial a} . \frac{\partial a}{\partial x} + \frac{\partial c}{\partial b} . \frac{\partial b}{\partial x})
$$</p>
<p>Therefore</p>
<p>$$
\frac{\partial d}{\partial x} = 2x + y
$$</p>
<p>And</p>
<p>$$
\frac{\partial d}{\partial y} = \frac{\partial d}{\partial c} . \frac{\partial c}{\partial y} = \frac{\partial d}{\partial c} . (\frac{\partial c}{\partial a} . \frac{\partial a}{\partial y} + \frac{\partial c}{\partial b} . \frac{\partial b}{\partial y})
$$</p>
<p>Therefore</p>
<p>$$
\frac{\partial d}{\partial y} = x
$$</p>
<p>Using the graph above, we can easily find the answers by tracing paths from $d$ to $x$ or $y$. Multiply the weights along each path and then sum the results from the different paths.</p>
<p>Therefore, the interest of automatic differentiation is in its ability to efficiently compute gradients for complex functions. It automates the application of the chain rule, ensuring accurate and fast derivatives.</p>
<h2 id="implementation">Implementation</h2>
<h3 id="n-dimensional-array">N-dimensional array</h3>
<p>Before we can implement the Autograd library, we need to create an N-dimensional array (ndarray) library.</p>
<p>The C structure for n-dimensional arrays will include the array&rsquo;s dimension, total size, shape (size of each dimension), and a pointer to the data. The data type is configurable via a macro.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-c" data-lang="c"><span style="display:flex;"><span><span style="color:#999;font-weight:bold;font-style:italic">#define NDARRAY_TYPE double
</span></span></span><span style="display:flex;"><span><span style="color:#999;font-weight:bold;font-style:italic"></span>
</span></span><span style="display:flex;"><span><span style="color:#000;font-weight:bold">typedef</span> <span style="color:#000;font-weight:bold">struct</span> ndarray {
</span></span><span style="display:flex;"><span>  <span style="color:#458;font-weight:bold">int</span> dim;
</span></span><span style="display:flex;"><span>  <span style="color:#458;font-weight:bold">int</span> size;
</span></span><span style="display:flex;"><span>  <span style="color:#458;font-weight:bold">int</span> <span style="color:#000;font-weight:bold">*</span>shape;
</span></span><span style="display:flex;"><span>  NDARRAY_TYPE <span style="color:#000;font-weight:bold">*</span>data;
</span></span><span style="display:flex;"><span>} ndarray;
</span></span></code></pre></div><p>Internally, we are representing n-dimensional arrays by a 1D array since it simplifies memory management and access. We can access elements in an n-dimensional array using the shape array. This approach allows us to avoid the need for complex nested loops and makes operations like slicing and reshaping straightforward and performant.</p>
<p>We need functions that operate on the ndarrays, we can distinguish between 3 types of operations:</p>
<ul>
<li><strong>Unary operations</strong>: operations that work on a single ndarray, transforming its elements via element-wise mathematical functions:</li>
</ul>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-c" data-lang="c"><span style="display:flex;"><span>ndarray <span style="color:#000;font-weight:bold">*</span><span style="color:#900;font-weight:bold">unary_op_ndarray</span>(ndarray <span style="color:#000;font-weight:bold">*</span>arr, <span style="color:#900;font-weight:bold">NDARRAY_TYPE</span> (<span style="color:#000;font-weight:bold">*</span>op)(NDARRAY_TYPE)) {
</span></span><span style="display:flex;"><span>  ndarray <span style="color:#000;font-weight:bold">*</span>n_arr <span style="color:#000;font-weight:bold">=</span> (ndarray <span style="color:#000;font-weight:bold">*</span>)<span style="color:#900;font-weight:bold">malloc</span>(<span style="color:#000;font-weight:bold">sizeof</span>(ndarray));
</span></span><span style="display:flex;"><span>  n_arr<span style="color:#000;font-weight:bold">-&gt;</span>dim <span style="color:#000;font-weight:bold">=</span> arr<span style="color:#000;font-weight:bold">-&gt;</span>dim;
</span></span><span style="display:flex;"><span>  n_arr<span style="color:#000;font-weight:bold">-&gt;</span>size <span style="color:#000;font-weight:bold">=</span> arr<span style="color:#000;font-weight:bold">-&gt;</span>size;
</span></span><span style="display:flex;"><span>  n_arr<span style="color:#000;font-weight:bold">-&gt;</span>shape <span style="color:#000;font-weight:bold">=</span> (<span style="color:#458;font-weight:bold">int</span> <span style="color:#000;font-weight:bold">*</span>)<span style="color:#900;font-weight:bold">malloc</span>(arr<span style="color:#000;font-weight:bold">-&gt;</span>dim <span style="color:#000;font-weight:bold">*</span> <span style="color:#000;font-weight:bold">sizeof</span>(<span style="color:#458;font-weight:bold">int</span>));
</span></span><span style="display:flex;"><span>  <span style="color:#000;font-weight:bold">for</span> (<span style="color:#458;font-weight:bold">int</span> i <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">0</span>; i <span style="color:#000;font-weight:bold">&lt;</span> arr<span style="color:#000;font-weight:bold">-&gt;</span>dim; i<span style="color:#000;font-weight:bold">++</span>) {
</span></span><span style="display:flex;"><span>    n_arr<span style="color:#000;font-weight:bold">-&gt;</span>shape[i] <span style="color:#000;font-weight:bold">=</span> arr<span style="color:#000;font-weight:bold">-&gt;</span>shape[i];
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>  n_arr<span style="color:#000;font-weight:bold">-&gt;</span>data <span style="color:#000;font-weight:bold">=</span> (NDARRAY_TYPE <span style="color:#000;font-weight:bold">*</span>)<span style="color:#900;font-weight:bold">malloc</span>(arr<span style="color:#000;font-weight:bold">-&gt;</span>size <span style="color:#000;font-weight:bold">*</span> <span style="color:#000;font-weight:bold">sizeof</span>(NDARRAY_TYPE));
</span></span><span style="display:flex;"><span>  <span style="color:#000;font-weight:bold">for</span> (<span style="color:#458;font-weight:bold">int</span> i <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">0</span>; i <span style="color:#000;font-weight:bold">&lt;</span> arr<span style="color:#000;font-weight:bold">-&gt;</span>size; i<span style="color:#000;font-weight:bold">++</span>) {
</span></span><span style="display:flex;"><span>    n_arr<span style="color:#000;font-weight:bold">-&gt;</span>data[i] <span style="color:#000;font-weight:bold">=</span> <span style="color:#900;font-weight:bold">op</span>(arr<span style="color:#000;font-weight:bold">-&gt;</span>data[i]);
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#000;font-weight:bold">return</span> n_arr;
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><ul>
<li><strong>Binary operations</strong>: operations that combine two ndarrays element-wise. These include arithmetic operations (addition, subtraction, multiplication, division). We will also support broadcasting, which allows operations on ndarrays of different but compatible shapes, making it possible to add a vector to a matrix for example which adds the vector to each row of the matrix.</li>
</ul>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-c" data-lang="c"><span style="display:flex;"><span>ndarray <span style="color:#000;font-weight:bold">*</span><span style="color:#900;font-weight:bold">binary_op_ndarray</span>(ndarray <span style="color:#000;font-weight:bold">*</span>arr1, ndarray <span style="color:#000;font-weight:bold">*</span>arr2,
</span></span><span style="display:flex;"><span>                           <span style="color:#900;font-weight:bold">NDARRAY_TYPE</span> (<span style="color:#000;font-weight:bold">*</span>op)(NDARRAY_TYPE, NDARRAY_TYPE)) {
</span></span><span style="display:flex;"><span>  <span style="color:#000;font-weight:bold">if</span> (arr1<span style="color:#000;font-weight:bold">-&gt;</span>dim <span style="color:#000;font-weight:bold">!=</span> arr2<span style="color:#000;font-weight:bold">-&gt;</span>dim) {
</span></span><span style="display:flex;"><span>    <span style="color:#900;font-weight:bold">printf</span>(<span style="color:#d14">&#34;Incompatible dimensions&#34;</span>);
</span></span><span style="display:flex;"><span>    <span style="color:#000;font-weight:bold">return</span> <span style="color:#0086b3">NULL</span>;
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>  <span style="color:#000;font-weight:bold">for</span> (<span style="color:#458;font-weight:bold">int</span> i <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">0</span>; i <span style="color:#000;font-weight:bold">&lt;</span> arr1<span style="color:#000;font-weight:bold">-&gt;</span>dim; i<span style="color:#000;font-weight:bold">++</span>) {
</span></span><span style="display:flex;"><span>    <span style="color:#000;font-weight:bold">if</span> ((arr1<span style="color:#000;font-weight:bold">-&gt;</span>shape[i] <span style="color:#000;font-weight:bold">!=</span> arr2<span style="color:#000;font-weight:bold">-&gt;</span>shape[i]) <span style="color:#000;font-weight:bold">&amp;&amp;</span>
</span></span><span style="display:flex;"><span>        (arr1<span style="color:#000;font-weight:bold">-&gt;</span>shape[i] <span style="color:#000;font-weight:bold">!=</span> <span style="color:#099">1</span> <span style="color:#000;font-weight:bold">&amp;&amp;</span> arr2<span style="color:#000;font-weight:bold">-&gt;</span>shape[i] <span style="color:#000;font-weight:bold">!=</span> <span style="color:#099">1</span>)) {
</span></span><span style="display:flex;"><span>      <span style="color:#900;font-weight:bold">printf</span>(<span style="color:#d14">&#34;Incompatible dimensions&#34;</span>);
</span></span><span style="display:flex;"><span>      <span style="color:#000;font-weight:bold">return</span> <span style="color:#0086b3">NULL</span>;
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>  <span style="color:#458;font-weight:bold">int</span> dim <span style="color:#000;font-weight:bold">=</span> arr1<span style="color:#000;font-weight:bold">-&gt;</span>dim;
</span></span><span style="display:flex;"><span>  <span style="color:#458;font-weight:bold">int</span> <span style="color:#000;font-weight:bold">*</span>shape <span style="color:#000;font-weight:bold">=</span> (<span style="color:#458;font-weight:bold">int</span> <span style="color:#000;font-weight:bold">*</span>)<span style="color:#900;font-weight:bold">malloc</span>(dim <span style="color:#000;font-weight:bold">*</span> <span style="color:#000;font-weight:bold">sizeof</span>(<span style="color:#458;font-weight:bold">int</span>));
</span></span><span style="display:flex;"><span>  <span style="color:#000;font-weight:bold">for</span> (<span style="color:#458;font-weight:bold">int</span> i <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">0</span>; i <span style="color:#000;font-weight:bold">&lt;</span> dim; i<span style="color:#000;font-weight:bold">++</span>) {
</span></span><span style="display:flex;"><span>    <span style="color:#458;font-weight:bold">int</span> shape1 <span style="color:#000;font-weight:bold">=</span> arr1<span style="color:#000;font-weight:bold">-&gt;</span>shape[i];
</span></span><span style="display:flex;"><span>    <span style="color:#458;font-weight:bold">int</span> shape2 <span style="color:#000;font-weight:bold">=</span> arr2<span style="color:#000;font-weight:bold">-&gt;</span>shape[i];
</span></span><span style="display:flex;"><span>    shape[i] <span style="color:#000;font-weight:bold">=</span> shape1 <span style="color:#000;font-weight:bold">&gt;</span> shape2 <span style="color:#000;font-weight:bold">?</span> <span style="color:#900;font-weight:bold">shape1</span> : shape2;
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>  ndarray <span style="color:#000;font-weight:bold">*</span>arr <span style="color:#000;font-weight:bold">=</span> <span style="color:#900;font-weight:bold">zeros_ndarray</span>(dim, shape);
</span></span><span style="display:flex;"><span>  <span style="color:#900;font-weight:bold">free</span>(shape);
</span></span><span style="display:flex;"><span>  <span style="color:#000;font-weight:bold">for</span> (<span style="color:#458;font-weight:bold">int</span> i <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">0</span>; i <span style="color:#000;font-weight:bold">&lt;</span> arr<span style="color:#000;font-weight:bold">-&gt;</span>size; i<span style="color:#000;font-weight:bold">++</span>) {
</span></span><span style="display:flex;"><span>    <span style="color:#458;font-weight:bold">int</span> idx1 <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">0</span>, idx2 <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">0</span>, temp <span style="color:#000;font-weight:bold">=</span> i, stride1 <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">1</span>, stride2 <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">1</span>;
</span></span><span style="display:flex;"><span>    <span style="color:#000;font-weight:bold">for</span> (<span style="color:#458;font-weight:bold">int</span> j <span style="color:#000;font-weight:bold">=</span> arr<span style="color:#000;font-weight:bold">-&gt;</span>dim <span style="color:#000;font-weight:bold">-</span> <span style="color:#099">1</span>; j <span style="color:#000;font-weight:bold">&gt;=</span> <span style="color:#099">0</span>; j<span style="color:#000;font-weight:bold">--</span>) {
</span></span><span style="display:flex;"><span>      <span style="color:#458;font-weight:bold">int</span> shape1 <span style="color:#000;font-weight:bold">=</span> arr1<span style="color:#000;font-weight:bold">-&gt;</span>shape[j];
</span></span><span style="display:flex;"><span>      <span style="color:#458;font-weight:bold">int</span> shape2 <span style="color:#000;font-weight:bold">=</span> arr2<span style="color:#000;font-weight:bold">-&gt;</span>shape[j];
</span></span><span style="display:flex;"><span>      idx1 <span style="color:#000;font-weight:bold">+=</span> (temp <span style="color:#000;font-weight:bold">%</span> shape1) <span style="color:#000;font-weight:bold">*</span> stride1;
</span></span><span style="display:flex;"><span>      idx2 <span style="color:#000;font-weight:bold">+=</span> (temp <span style="color:#000;font-weight:bold">%</span> shape2) <span style="color:#000;font-weight:bold">*</span> stride2;
</span></span><span style="display:flex;"><span>      stride1 <span style="color:#000;font-weight:bold">*=</span> shape1;
</span></span><span style="display:flex;"><span>      stride2 <span style="color:#000;font-weight:bold">*=</span> shape2;
</span></span><span style="display:flex;"><span>      temp <span style="color:#000;font-weight:bold">/=</span> (shape1 <span style="color:#000;font-weight:bold">&gt;</span> shape2 <span style="color:#000;font-weight:bold">?</span> <span style="color:#900;font-weight:bold">shape1</span> : shape2);
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>    arr<span style="color:#000;font-weight:bold">-&gt;</span>data[i] <span style="color:#000;font-weight:bold">=</span>
</span></span><span style="display:flex;"><span>        <span style="color:#900;font-weight:bold">op</span>(arr1<span style="color:#000;font-weight:bold">-&gt;</span>data[idx1 <span style="color:#000;font-weight:bold">%</span> arr1<span style="color:#000;font-weight:bold">-&gt;</span>size], arr2<span style="color:#000;font-weight:bold">-&gt;</span>data[idx2 <span style="color:#000;font-weight:bold">%</span> arr2<span style="color:#000;font-weight:bold">-&gt;</span>size]);
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>  <span style="color:#000;font-weight:bold">return</span> arr;
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><ul>
<li><strong>Reduce operations</strong>: operations that collapse one or more dimensions of an ndarray, producing a result with fewer dimensions. Examples include sum, mean, max, and min along specified axes.</li>
</ul>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-c" data-lang="c"><span style="display:flex;"><span><span style="color:#000;font-weight:bold">static</span> <span style="color:#458;font-weight:bold">int</span> <span style="color:#900;font-weight:bold">get_offset</span>(ndarray <span style="color:#000;font-weight:bold">*</span>arr, <span style="color:#000;font-weight:bold">const</span> <span style="color:#458;font-weight:bold">int</span> <span style="color:#000;font-weight:bold">*</span>position, <span style="color:#458;font-weight:bold">int</span> pdim) {
</span></span><span style="display:flex;"><span>  <span style="color:#458;font-weight:bold">unsigned</span> <span style="color:#458;font-weight:bold">int</span> offset <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">0</span>;
</span></span><span style="display:flex;"><span>  <span style="color:#458;font-weight:bold">unsigned</span> <span style="color:#458;font-weight:bold">int</span> len <span style="color:#000;font-weight:bold">=</span> arr<span style="color:#000;font-weight:bold">-&gt;</span>size;
</span></span><span style="display:flex;"><span>  <span style="color:#000;font-weight:bold">for</span> (<span style="color:#458;font-weight:bold">int</span> i <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">0</span>; i <span style="color:#000;font-weight:bold">&lt;</span> pdim; i<span style="color:#000;font-weight:bold">++</span>) {
</span></span><span style="display:flex;"><span>    len <span style="color:#000;font-weight:bold">/=</span> arr<span style="color:#000;font-weight:bold">-&gt;</span>shape[i];
</span></span><span style="display:flex;"><span>    offset <span style="color:#000;font-weight:bold">+=</span> position[i] <span style="color:#000;font-weight:bold">*</span> len;
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>  <span style="color:#000;font-weight:bold">return</span> offset;
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#458;font-weight:bold">void</span> <span style="color:#900;font-weight:bold">reduce_ndarray_helper</span>(ndarray <span style="color:#000;font-weight:bold">*</span>arr, ndarray <span style="color:#000;font-weight:bold">*</span>n_arr, <span style="color:#458;font-weight:bold">int</span> <span style="color:#000;font-weight:bold">*</span>position,
</span></span><span style="display:flex;"><span>                           <span style="color:#900;font-weight:bold">NDARRAY_TYPE</span> (<span style="color:#000;font-weight:bold">*</span>op)(NDARRAY_TYPE, NDARRAY_TYPE),
</span></span><span style="display:flex;"><span>                           <span style="color:#458;font-weight:bold">int</span> axis, <span style="color:#458;font-weight:bold">int</span> dim) {
</span></span><span style="display:flex;"><span>  <span style="color:#000;font-weight:bold">if</span> (dim <span style="color:#000;font-weight:bold">&gt;=</span> arr<span style="color:#000;font-weight:bold">-&gt;</span>dim) {
</span></span><span style="display:flex;"><span>    <span style="color:#458;font-weight:bold">int</span> rdim <span style="color:#000;font-weight:bold">=</span> n_arr<span style="color:#000;font-weight:bold">-&gt;</span>dim;
</span></span><span style="display:flex;"><span>    <span style="color:#458;font-weight:bold">int</span> n_position[rdim];
</span></span><span style="display:flex;"><span>    <span style="color:#000;font-weight:bold">for</span> (<span style="color:#458;font-weight:bold">int</span> i <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">0</span>; i <span style="color:#000;font-weight:bold">&lt;</span> rdim; i<span style="color:#000;font-weight:bold">++</span>) {
</span></span><span style="display:flex;"><span>      n_position[i] <span style="color:#000;font-weight:bold">=</span> (i <span style="color:#000;font-weight:bold">==</span> axis) <span style="color:#000;font-weight:bold">?</span> <span style="color:#099">0</span> <span style="color:#000;font-weight:bold">:</span> position[i];
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>    <span style="color:#458;font-weight:bold">int</span> offset_arr <span style="color:#000;font-weight:bold">=</span> <span style="color:#900;font-weight:bold">get_offset</span>(arr, position, arr<span style="color:#000;font-weight:bold">-&gt;</span>dim);
</span></span><span style="display:flex;"><span>    <span style="color:#458;font-weight:bold">int</span> offset_narr <span style="color:#000;font-weight:bold">=</span> <span style="color:#900;font-weight:bold">get_offset</span>(n_arr, n_position, n_arr<span style="color:#000;font-weight:bold">-&gt;</span>dim);
</span></span><span style="display:flex;"><span>    n_arr<span style="color:#000;font-weight:bold">-&gt;</span>data[offset_narr] <span style="color:#000;font-weight:bold">=</span>
</span></span><span style="display:flex;"><span>        (dim <span style="color:#000;font-weight:bold">==</span> axis) <span style="color:#000;font-weight:bold">?</span> arr<span style="color:#000;font-weight:bold">-&gt;</span>data[offset_arr]
</span></span><span style="display:flex;"><span>                      <span style="color:#000;font-weight:bold">:</span> <span style="color:#900;font-weight:bold">op</span>(n_arr<span style="color:#000;font-weight:bold">-&gt;</span>data[offset_narr], arr<span style="color:#000;font-weight:bold">-&gt;</span>data[offset_arr]);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#000;font-weight:bold">return</span>;
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>  <span style="color:#000;font-weight:bold">for</span> (<span style="color:#458;font-weight:bold">int</span> i <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">0</span>; i <span style="color:#000;font-weight:bold">&lt;</span> arr<span style="color:#000;font-weight:bold">-&gt;</span>shape[dim]; i<span style="color:#000;font-weight:bold">++</span>) {
</span></span><span style="display:flex;"><span>    position[dim] <span style="color:#000;font-weight:bold">=</span> i;
</span></span><span style="display:flex;"><span>    <span style="color:#900;font-weight:bold">reduce_ndarray_helper</span>(arr, n_arr, position, op, axis, dim <span style="color:#000;font-weight:bold">+</span> <span style="color:#099">1</span>);
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>ndarray <span style="color:#000;font-weight:bold">*</span><span style="color:#900;font-weight:bold">reduce_ndarray</span>(ndarray <span style="color:#000;font-weight:bold">*</span>arr,
</span></span><span style="display:flex;"><span>                        <span style="color:#900;font-weight:bold">NDARRAY_TYPE</span> (<span style="color:#000;font-weight:bold">*</span>op)(NDARRAY_TYPE, NDARRAY_TYPE),
</span></span><span style="display:flex;"><span>                        <span style="color:#458;font-weight:bold">int</span> axis, NDARRAY_TYPE initial_value) {
</span></span><span style="display:flex;"><span>  <span style="color:#458;font-weight:bold">int</span> <span style="color:#000;font-weight:bold">*</span>shape <span style="color:#000;font-weight:bold">=</span> (<span style="color:#458;font-weight:bold">int</span> <span style="color:#000;font-weight:bold">*</span>)<span style="color:#900;font-weight:bold">malloc</span>(arr<span style="color:#000;font-weight:bold">-&gt;</span>dim <span style="color:#000;font-weight:bold">*</span> <span style="color:#000;font-weight:bold">sizeof</span>(<span style="color:#458;font-weight:bold">int</span>));
</span></span><span style="display:flex;"><span>  <span style="color:#000;font-weight:bold">for</span> (<span style="color:#458;font-weight:bold">int</span> i <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">0</span>; i <span style="color:#000;font-weight:bold">&lt;</span> arr<span style="color:#000;font-weight:bold">-&gt;</span>dim; i<span style="color:#000;font-weight:bold">++</span>) {
</span></span><span style="display:flex;"><span>    shape[i] <span style="color:#000;font-weight:bold">=</span> (i <span style="color:#000;font-weight:bold">==</span> axis) <span style="color:#000;font-weight:bold">?</span> <span style="color:#099">1</span> <span style="color:#000;font-weight:bold">:</span> arr<span style="color:#000;font-weight:bold">-&gt;</span>shape[i];
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>  ndarray <span style="color:#000;font-weight:bold">*</span>n_arr <span style="color:#000;font-weight:bold">=</span> <span style="color:#900;font-weight:bold">full_ndarray</span>(arr<span style="color:#000;font-weight:bold">-&gt;</span>dim, shape, initial_value);
</span></span><span style="display:flex;"><span>  <span style="color:#900;font-weight:bold">free</span>(shape);
</span></span><span style="display:flex;"><span>  <span style="color:#458;font-weight:bold">int</span> position[arr<span style="color:#000;font-weight:bold">-&gt;</span>dim];
</span></span><span style="display:flex;"><span>  <span style="color:#900;font-weight:bold">reduce_ndarray_helper</span>(arr, n_arr, position, op, axis, <span style="color:#099">0</span>);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#000;font-weight:bold">return</span> n_arr;
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>For more info check <a href="https://github.com/smdaa/teeny-autograd-c/blob/main/src/ndarray.c">ndarray.c</a></p>
<h3 id="variable-node">Variable node</h3>
<p>Let us now implement the structure that will make it possible to use Autograd. We will define a representation of a node in the autograd graph:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-c" data-lang="c"><span style="display:flex;"><span><span style="color:#000;font-weight:bold">typedef</span> <span style="color:#000;font-weight:bold">struct</span> variable {
</span></span><span style="display:flex;"><span>  ndarray <span style="color:#000;font-weight:bold">*</span>val;
</span></span><span style="display:flex;"><span>  ndarray <span style="color:#000;font-weight:bold">*</span>grad;
</span></span><span style="display:flex;"><span>  <span style="color:#000;font-weight:bold">struct</span> variable <span style="color:#000;font-weight:bold">**</span>children;
</span></span><span style="display:flex;"><span>  <span style="color:#458;font-weight:bold">int</span> n_children;
</span></span><span style="display:flex;"><span>  <span style="color:#458;font-weight:bold">void</span> (<span style="color:#000;font-weight:bold">*</span>backward)(<span style="color:#000;font-weight:bold">struct</span> variable <span style="color:#000;font-weight:bold">*</span>);
</span></span><span style="display:flex;"><span>  <span style="color:#458;font-weight:bold">int</span> ref_count;
</span></span><span style="display:flex;"><span>} variable;
</span></span></code></pre></div><ul>
<li><code>ndarray *val</code> : This pointer holds the actual value of the variable stored as a ndarray.</li>
<li><code>ndarray *grad</code> : This pointer stores the gradient of the variable, which is essential for backpropagation.</li>
<li><code>struct variable **children</code> : A pointer to an array of pointers to other variables. These represent the variables that depend on this one in the computational graph.</li>
<li><code>int n_children</code> : The number of child variables, used to keep track of the size of the children array.</li>
<li><code>void (*backward)(struct variable *)</code> : A function pointer to the backward operation for this variable. This function will be called during backpropagation to compute gradients.</li>
<li><code>int ref_count</code> : A reference counter for memory management, useful for determining when the variable can be safely deallocated.</li>
</ul>
<p>we will also define operations on variables :</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-c" data-lang="c"><span style="display:flex;"><span>variable <span style="color:#000;font-weight:bold">*</span><span style="color:#900;font-weight:bold">add_variable</span>(variable <span style="color:#000;font-weight:bold">*</span>var1, variable <span style="color:#000;font-weight:bold">*</span>var2);
</span></span><span style="display:flex;"><span>variable <span style="color:#000;font-weight:bold">*</span><span style="color:#900;font-weight:bold">subtract_variable</span>(variable <span style="color:#000;font-weight:bold">*</span>var1, variable <span style="color:#000;font-weight:bold">*</span>var2);
</span></span><span style="display:flex;"><span>variable <span style="color:#000;font-weight:bold">*</span><span style="color:#900;font-weight:bold">multiply_variable</span>(variable <span style="color:#000;font-weight:bold">*</span>var1, variable <span style="color:#000;font-weight:bold">*</span>var2);
</span></span><span style="display:flex;"><span>variable <span style="color:#000;font-weight:bold">*</span><span style="color:#900;font-weight:bold">divide_variable</span>(variable <span style="color:#000;font-weight:bold">*</span>var1, variable <span style="color:#000;font-weight:bold">*</span>var2);
</span></span><span style="display:flex;"><span>variable <span style="color:#000;font-weight:bold">*</span><span style="color:#900;font-weight:bold">power_variable</span>(variable <span style="color:#000;font-weight:bold">*</span>var1, variable <span style="color:#000;font-weight:bold">*</span>var2);
</span></span><span style="display:flex;"><span>variable <span style="color:#000;font-weight:bold">*</span><span style="color:#900;font-weight:bold">negate_variable</span>(variable <span style="color:#000;font-weight:bold">*</span>var);
</span></span><span style="display:flex;"><span>variable <span style="color:#000;font-weight:bold">*</span><span style="color:#900;font-weight:bold">exp_variable</span>(variable <span style="color:#000;font-weight:bold">*</span>var);
</span></span><span style="display:flex;"><span>variable <span style="color:#000;font-weight:bold">*</span><span style="color:#900;font-weight:bold">log_variable</span>(variable <span style="color:#000;font-weight:bold">*</span>var);
</span></span><span style="display:flex;"><span>variable <span style="color:#000;font-weight:bold">*</span><span style="color:#900;font-weight:bold">sum_variable</span>(variable <span style="color:#000;font-weight:bold">*</span>var, <span style="color:#458;font-weight:bold">int</span> axis);
</span></span><span style="display:flex;"><span>variable <span style="color:#000;font-weight:bold">*</span><span style="color:#900;font-weight:bold">relu_variable</span>(variable <span style="color:#000;font-weight:bold">*</span>var);
</span></span><span style="display:flex;"><span>variable <span style="color:#000;font-weight:bold">*</span><span style="color:#900;font-weight:bold">sigmoid_variable</span>(variable <span style="color:#000;font-weight:bold">*</span>var);
</span></span><span style="display:flex;"><span>variable <span style="color:#000;font-weight:bold">*</span><span style="color:#900;font-weight:bold">softmax_variable</span>(variable <span style="color:#000;font-weight:bold">*</span>var, <span style="color:#458;font-weight:bold">int</span> axis);
</span></span><span style="display:flex;"><span>variable <span style="color:#000;font-weight:bold">*</span><span style="color:#900;font-weight:bold">tanh_variable</span>(variable <span style="color:#000;font-weight:bold">*</span>var);
</span></span><span style="display:flex;"><span>variable <span style="color:#000;font-weight:bold">*</span><span style="color:#900;font-weight:bold">matmul_variable</span>(variable <span style="color:#000;font-weight:bold">*</span>var1, variable <span style="color:#000;font-weight:bold">*</span>var2);
</span></span></code></pre></div><p>The idea is to use these building blocks to build what ever function we want, and then when we would want to compute the gradient of set function we would simple call on the <code>backward</code> function:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-c" data-lang="c"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#458;font-weight:bold">void</span> <span style="color:#900;font-weight:bold">build_topology</span>(variable <span style="color:#000;font-weight:bold">*</span>var, variable <span style="color:#000;font-weight:bold">***</span>topology, <span style="color:#458;font-weight:bold">int</span> <span style="color:#000;font-weight:bold">*</span>topology_size,
</span></span><span style="display:flex;"><span>                    variable <span style="color:#000;font-weight:bold">***</span>visited, <span style="color:#458;font-weight:bold">int</span> <span style="color:#000;font-weight:bold">*</span>visited_size) {
</span></span><span style="display:flex;"><span>  <span style="color:#000;font-weight:bold">for</span> (<span style="color:#458;font-weight:bold">int</span> i <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">0</span>; i <span style="color:#000;font-weight:bold">&lt;</span> <span style="color:#000;font-weight:bold">*</span>visited_size; <span style="color:#000;font-weight:bold">++</span>i) {
</span></span><span style="display:flex;"><span>    <span style="color:#000;font-weight:bold">if</span> ((<span style="color:#000;font-weight:bold">*</span>visited)[i] <span style="color:#000;font-weight:bold">==</span> var) {
</span></span><span style="display:flex;"><span>      <span style="color:#000;font-weight:bold">return</span>;
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>  <span style="color:#000;font-weight:bold">*</span>visited <span style="color:#000;font-weight:bold">=</span>
</span></span><span style="display:flex;"><span>      (variable <span style="color:#000;font-weight:bold">**</span>)<span style="color:#900;font-weight:bold">realloc</span>(<span style="color:#000;font-weight:bold">*</span>visited, (<span style="color:#000;font-weight:bold">*</span>visited_size <span style="color:#000;font-weight:bold">+</span> <span style="color:#099">1</span>) <span style="color:#000;font-weight:bold">*</span> <span style="color:#000;font-weight:bold">sizeof</span>(variable <span style="color:#000;font-weight:bold">*</span>));
</span></span><span style="display:flex;"><span>  (<span style="color:#000;font-weight:bold">*</span>visited)[<span style="color:#000;font-weight:bold">*</span>visited_size] <span style="color:#000;font-weight:bold">=</span> var;
</span></span><span style="display:flex;"><span>  (<span style="color:#000;font-weight:bold">*</span>visited_size)<span style="color:#000;font-weight:bold">++</span>;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#000;font-weight:bold">for</span> (<span style="color:#458;font-weight:bold">int</span> i <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">0</span>; i <span style="color:#000;font-weight:bold">&lt;</span> var<span style="color:#000;font-weight:bold">-&gt;</span>n_children; <span style="color:#000;font-weight:bold">++</span>i) {
</span></span><span style="display:flex;"><span>    <span style="color:#900;font-weight:bold">build_topology</span>(var<span style="color:#000;font-weight:bold">-&gt;</span>children[i], topology, topology_size, visited,
</span></span><span style="display:flex;"><span>                   visited_size);
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>  <span style="color:#000;font-weight:bold">*</span>topology <span style="color:#000;font-weight:bold">=</span> (variable <span style="color:#000;font-weight:bold">**</span>)<span style="color:#900;font-weight:bold">realloc</span>(<span style="color:#000;font-weight:bold">*</span>topology,
</span></span><span style="display:flex;"><span>                                   (<span style="color:#000;font-weight:bold">*</span>topology_size <span style="color:#000;font-weight:bold">+</span> <span style="color:#099">1</span>) <span style="color:#000;font-weight:bold">*</span> <span style="color:#000;font-weight:bold">sizeof</span>(variable <span style="color:#000;font-weight:bold">*</span>));
</span></span><span style="display:flex;"><span>  (<span style="color:#000;font-weight:bold">*</span>topology)[<span style="color:#000;font-weight:bold">*</span>topology_size] <span style="color:#000;font-weight:bold">=</span> var;
</span></span><span style="display:flex;"><span>  (<span style="color:#000;font-weight:bold">*</span>topology_size)<span style="color:#000;font-weight:bold">++</span>;
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#458;font-weight:bold">void</span> <span style="color:#900;font-weight:bold">backward_variable</span>(variable <span style="color:#000;font-weight:bold">*</span>root_var) {
</span></span><span style="display:flex;"><span>  variable <span style="color:#000;font-weight:bold">**</span>topology <span style="color:#000;font-weight:bold">=</span> <span style="color:#0086b3">NULL</span>;
</span></span><span style="display:flex;"><span>  <span style="color:#458;font-weight:bold">int</span> topology_size <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">0</span>;
</span></span><span style="display:flex;"><span>  variable <span style="color:#000;font-weight:bold">**</span>visited <span style="color:#000;font-weight:bold">=</span> <span style="color:#0086b3">NULL</span>;
</span></span><span style="display:flex;"><span>  <span style="color:#458;font-weight:bold">int</span> visited_size <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">0</span>;
</span></span><span style="display:flex;"><span>  <span style="color:#900;font-weight:bold">build_topology</span>(root_var, <span style="color:#000;font-weight:bold">&amp;</span>topology, <span style="color:#000;font-weight:bold">&amp;</span>topology_size, <span style="color:#000;font-weight:bold">&amp;</span>visited, <span style="color:#000;font-weight:bold">&amp;</span>visited_size);
</span></span><span style="display:flex;"><span>  <span style="color:#000;font-weight:bold">for</span> (<span style="color:#458;font-weight:bold">int</span> i <span style="color:#000;font-weight:bold">=</span> topology_size <span style="color:#000;font-weight:bold">-</span> <span style="color:#099">1</span>; i <span style="color:#000;font-weight:bold">&gt;=</span> <span style="color:#099">0</span>; <span style="color:#000;font-weight:bold">--</span>i) {
</span></span><span style="display:flex;"><span>    <span style="color:#000;font-weight:bold">if</span> (topology[i]<span style="color:#000;font-weight:bold">-&gt;</span>backward) {
</span></span><span style="display:flex;"><span>      topology[i]<span style="color:#000;font-weight:bold">-&gt;</span><span style="color:#900;font-weight:bold">backward</span>(topology[i]);
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>  <span style="color:#900;font-weight:bold">free</span>(topology);
</span></span><span style="display:flex;"><span>  <span style="color:#900;font-weight:bold">free</span>(visited);
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>In other words, we implement a topological sorting algorithm to ensure that gradients are computed in the correct order, from the output back to the inputs. This allows for automatic differentiation of complex, nested functions by applying the chain rule systematically through the computational graph.</p>
<p>Let&rsquo;s took a deeper look into <code>log_variable</code> as an example:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-c" data-lang="c"><span style="display:flex;"><span>variable <span style="color:#000;font-weight:bold">*</span><span style="color:#900;font-weight:bold">log_variable</span>(variable <span style="color:#000;font-weight:bold">*</span>var) {
</span></span><span style="display:flex;"><span>  variable <span style="color:#000;font-weight:bold">*</span>n_var <span style="color:#000;font-weight:bold">=</span> (variable <span style="color:#000;font-weight:bold">*</span>)<span style="color:#900;font-weight:bold">malloc</span>(<span style="color:#000;font-weight:bold">sizeof</span>(variable));
</span></span><span style="display:flex;"><span>  n_var<span style="color:#000;font-weight:bold">-&gt;</span>val <span style="color:#000;font-weight:bold">=</span> <span style="color:#900;font-weight:bold">unary_op_ndarray</span>(var<span style="color:#000;font-weight:bold">-&gt;</span>val, log);
</span></span><span style="display:flex;"><span>  n_var<span style="color:#000;font-weight:bold">-&gt;</span>grad <span style="color:#000;font-weight:bold">=</span> <span style="color:#900;font-weight:bold">zeros_ndarray</span>(n_var<span style="color:#000;font-weight:bold">-&gt;</span>val<span style="color:#000;font-weight:bold">-&gt;</span>dim, n_var<span style="color:#000;font-weight:bold">-&gt;</span>val<span style="color:#000;font-weight:bold">-&gt;</span>shape);
</span></span><span style="display:flex;"><span>  n_var<span style="color:#000;font-weight:bold">-&gt;</span>children <span style="color:#000;font-weight:bold">=</span> (variable <span style="color:#000;font-weight:bold">**</span>)<span style="color:#900;font-weight:bold">malloc</span>(<span style="color:#000;font-weight:bold">sizeof</span>(variable <span style="color:#000;font-weight:bold">*</span>));
</span></span><span style="display:flex;"><span>  n_var<span style="color:#000;font-weight:bold">-&gt;</span>children[<span style="color:#099">0</span>] <span style="color:#000;font-weight:bold">=</span> var;
</span></span><span style="display:flex;"><span>  n_var<span style="color:#000;font-weight:bold">-&gt;</span>n_children <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">1</span>;
</span></span><span style="display:flex;"><span>  n_var<span style="color:#000;font-weight:bold">-&gt;</span>backward <span style="color:#000;font-weight:bold">=</span> log_backward;
</span></span><span style="display:flex;"><span>  n_var<span style="color:#000;font-weight:bold">-&gt;</span>ref_count <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">0</span>;
</span></span><span style="display:flex;"><span>  var<span style="color:#000;font-weight:bold">-&gt;</span>ref_count<span style="color:#000;font-weight:bold">++</span>;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#000;font-weight:bold">return</span> n_var;
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>The <code>log_variable</code> creates a new variable node that represents the natural logarithm of an input variable. It allocates memory for this new node, computes its value using the logarithm function, initializes its gradient to zero, and sets up the computational graph structure by linking it to its input (child) variable. The function also assigns the appropriate backward function for gradient computation during backpropagation.</p>
<p>If we look at <code>log_backward</code>:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-c" data-lang="c"><span style="display:flex;"><span><span style="color:#458;font-weight:bold">void</span> <span style="color:#900;font-weight:bold">log_backward</span>(variable <span style="color:#000;font-weight:bold">*</span>var) {
</span></span><span style="display:flex;"><span>  ndarray <span style="color:#000;font-weight:bold">*</span>place_holder;
</span></span><span style="display:flex;"><span>  ndarray <span style="color:#000;font-weight:bold">*</span>temp0;
</span></span><span style="display:flex;"><span>  ndarray <span style="color:#000;font-weight:bold">*</span>temp1;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  place_holder <span style="color:#000;font-weight:bold">=</span> var<span style="color:#000;font-weight:bold">-&gt;</span>children[<span style="color:#099">0</span>]<span style="color:#000;font-weight:bold">-&gt;</span>grad;
</span></span><span style="display:flex;"><span>  temp0 <span style="color:#000;font-weight:bold">=</span> <span style="color:#900;font-weight:bold">divide_scalar_ndarray</span>(var<span style="color:#000;font-weight:bold">-&gt;</span>children[<span style="color:#099">0</span>]<span style="color:#000;font-weight:bold">-&gt;</span>val, <span style="color:#099">1.0</span>);
</span></span><span style="display:flex;"><span>  temp1 <span style="color:#000;font-weight:bold">=</span> <span style="color:#900;font-weight:bold">multiply_ndarray_ndarray</span>(var<span style="color:#000;font-weight:bold">-&gt;</span>grad, temp0);
</span></span><span style="display:flex;"><span>  var<span style="color:#000;font-weight:bold">-&gt;</span>children[<span style="color:#099">0</span>]<span style="color:#000;font-weight:bold">-&gt;</span>grad <span style="color:#000;font-weight:bold">=</span> <span style="color:#900;font-weight:bold">add_ndarray_ndarray</span>(var<span style="color:#000;font-weight:bold">-&gt;</span>children[<span style="color:#099">0</span>]<span style="color:#000;font-weight:bold">-&gt;</span>grad, temp1);
</span></span><span style="display:flex;"><span>  <span style="color:#900;font-weight:bold">free_ndarray</span>(<span style="color:#000;font-weight:bold">&amp;</span>temp0);
</span></span><span style="display:flex;"><span>  <span style="color:#900;font-weight:bold">free_ndarray</span>(<span style="color:#000;font-weight:bold">&amp;</span>temp1);
</span></span><span style="display:flex;"><span>  <span style="color:#900;font-weight:bold">free_ndarray</span>(<span style="color:#000;font-weight:bold">&amp;</span>place_holder);
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>The <code>log_backward</code> function computes gradients for the natural logarithm operation in automatic differentiation. It applies the chain rule, using the fact that d/$\frac{d}{dx}ln(x) = \frac{1}{x}$. The function multiplies $\frac{1}{x}$ by the output gradient, adds this to the input&rsquo;s existing gradient.</p>
<p>For more info check <a href="https://github.com/smdaa/teeny-autograd-c/blob/main/src/variable.c">variable.c</a></p>
<h3 id="multilayer-perceptron">Multilayer perceptron</h3>
<p>Now that we have defined the variable structure, we have all we need to implement a Multilayer Perceptron (MLP), also known as a feedforward neural network.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-c" data-lang="c"><span style="display:flex;"><span><span style="color:#000;font-weight:bold">typedef</span> <span style="color:#000;font-weight:bold">struct</span> multilayer_perceptron {
</span></span><span style="display:flex;"><span>  <span style="color:#458;font-weight:bold">int</span> n_layers;
</span></span><span style="display:flex;"><span>  <span style="color:#458;font-weight:bold">int</span> batch_size;
</span></span><span style="display:flex;"><span>  <span style="color:#458;font-weight:bold">int</span> <span style="color:#000;font-weight:bold">*</span>in_sizes;
</span></span><span style="display:flex;"><span>  <span style="color:#458;font-weight:bold">int</span> <span style="color:#000;font-weight:bold">*</span>out_sizes;
</span></span><span style="display:flex;"><span>  variable <span style="color:#000;font-weight:bold">**</span>weights;
</span></span><span style="display:flex;"><span>  variable <span style="color:#000;font-weight:bold">**</span>bias;
</span></span><span style="display:flex;"><span>  variable <span style="color:#000;font-weight:bold">**</span>weights_copy;
</span></span><span style="display:flex;"><span>  variable <span style="color:#000;font-weight:bold">**</span>bias_copy;
</span></span><span style="display:flex;"><span>  activation_function <span style="color:#000;font-weight:bold">*</span>activations;
</span></span><span style="display:flex;"><span>  random_initialisation <span style="color:#000;font-weight:bold">*</span>random_initialisations;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>} multilayer_perceptron;
</span></span></code></pre></div><p>The <code>multilayer_perceptron</code> struct is designed to represent a neural network model with multiple layers. Here&rsquo;s a breakdown of its components:</p>
<ul>
<li><code>n_layers</code>: The number of layers in the MLP, excluding the input layer.</li>
<li><code>batch_size</code>: The size of the input data batches the MLP processes at once.</li>
<li><code>in_sizes</code>: An array containing the input sizes for each layer.</li>
<li><code>out_sizes</code>: An array containing the output sizes for each layer.</li>
<li><code>weights</code>: An array of pointers to variable structs representing the weight matrices for each layer.</li>
<li><code>bias</code>: An array of pointers to variable structs representing the bias vectors for each layer.</li>
<li><code>weights_copy</code>: Copies of the weight matrices, used for optimization purposes.</li>
<li><code>bias_copy</code>: Copies of the bias vectors, used for optimization purposes.</li>
<li><code>activations</code>: An array specifying the activation function used for each layer:</li>
</ul>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-c" data-lang="c"><span style="display:flex;"><span><span style="color:#000;font-weight:bold">typedef</span> <span style="color:#000;font-weight:bold">enum</span> {
</span></span><span style="display:flex;"><span>  LINEAR,
</span></span><span style="display:flex;"><span>  RELU,
</span></span><span style="display:flex;"><span>  SIGMOID,
</span></span><span style="display:flex;"><span>  SOFTMAX,
</span></span><span style="display:flex;"><span>  TANH,
</span></span><span style="display:flex;"><span>} activation_function;
</span></span></code></pre></div><ul>
<li><code>random_initialisations</code>: An array specifying the random initialization method for the weights of each layer:</li>
</ul>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-c" data-lang="c"><span style="display:flex;"><span><span style="color:#000;font-weight:bold">typedef</span> <span style="color:#000;font-weight:bold">enum</span> {
</span></span><span style="display:flex;"><span>  UNIFORM,
</span></span><span style="display:flex;"><span>  NORMAL,
</span></span><span style="display:flex;"><span>  TRUNCATED_NORMAL,
</span></span><span style="display:flex;"><span>} random_initialisation;
</span></span></code></pre></div><h4 id="the-forward-pass">The forward pass</h4>
<p>The forward pass is simple: We first multiply the batched data by the weights, then add the bias, and finally apply the activation function for each layer.</p>
<p><img src="/assets/building-an-autograd-library-from-scratch-in-c-for-simple-neural-networks/mlp-forward-pass.png" alt=""></p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-c" data-lang="c"><span style="display:flex;"><span>variable <span style="color:#000;font-weight:bold">*</span><span style="color:#900;font-weight:bold">forward_batch_multilayer_perceptron</span>(multilayer_perceptron <span style="color:#000;font-weight:bold">*</span>mlp,
</span></span><span style="display:flex;"><span>                                              variable <span style="color:#000;font-weight:bold">*</span>x_batch) {
</span></span><span style="display:flex;"><span>  variable <span style="color:#000;font-weight:bold">*</span>output <span style="color:#000;font-weight:bold">=</span> x_batch;
</span></span><span style="display:flex;"><span>  <span style="color:#000;font-weight:bold">for</span> (<span style="color:#458;font-weight:bold">int</span> i <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">0</span>; i <span style="color:#000;font-weight:bold">&lt;</span> mlp<span style="color:#000;font-weight:bold">-&gt;</span>n_layers; i<span style="color:#000;font-weight:bold">++</span>) {
</span></span><span style="display:flex;"><span>    output <span style="color:#000;font-weight:bold">=</span> <span style="color:#900;font-weight:bold">matmul_variable</span>(output, mlp<span style="color:#000;font-weight:bold">-&gt;</span>weights[i]);
</span></span><span style="display:flex;"><span>    output <span style="color:#000;font-weight:bold">=</span> <span style="color:#900;font-weight:bold">add_variable</span>(output, mlp<span style="color:#000;font-weight:bold">-&gt;</span>bias[i]);
</span></span><span style="display:flex;"><span>    <span style="color:#000;font-weight:bold">switch</span> (mlp<span style="color:#000;font-weight:bold">-&gt;</span>activations[i]) {
</span></span><span style="display:flex;"><span>    <span style="color:#000;font-weight:bold">case</span> <span style="color:#900;font-weight:bold">LINEAR</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#000;font-weight:bold">break</span>;
</span></span><span style="display:flex;"><span>    <span style="color:#000;font-weight:bold">case</span> <span style="color:#900;font-weight:bold">RELU</span>:
</span></span><span style="display:flex;"><span>      output <span style="color:#000;font-weight:bold">=</span> <span style="color:#900;font-weight:bold">relu_variable</span>(output);
</span></span><span style="display:flex;"><span>      <span style="color:#000;font-weight:bold">break</span>;
</span></span><span style="display:flex;"><span>    <span style="color:#000;font-weight:bold">case</span> <span style="color:#900;font-weight:bold">SIGMOID</span>:
</span></span><span style="display:flex;"><span>      output <span style="color:#000;font-weight:bold">=</span> <span style="color:#900;font-weight:bold">sigmoid_variable</span>(output);
</span></span><span style="display:flex;"><span>      <span style="color:#000;font-weight:bold">break</span>;
</span></span><span style="display:flex;"><span>    <span style="color:#000;font-weight:bold">case</span> <span style="color:#900;font-weight:bold">SOFTMAX</span>:
</span></span><span style="display:flex;"><span>      output <span style="color:#000;font-weight:bold">=</span> <span style="color:#900;font-weight:bold">softmax_variable</span>(output, <span style="color:#099">1</span>);
</span></span><span style="display:flex;"><span>      <span style="color:#000;font-weight:bold">break</span>;
</span></span><span style="display:flex;"><span>    <span style="color:#000;font-weight:bold">case</span> <span style="color:#900;font-weight:bold">TANH</span>:
</span></span><span style="display:flex;"><span>      output <span style="color:#000;font-weight:bold">=</span> <span style="color:#900;font-weight:bold">tanh_variable</span>(output);
</span></span><span style="display:flex;"><span>      <span style="color:#000;font-weight:bold">break</span>;
</span></span><span style="display:flex;"><span>    <span style="color:#000;font-weight:bold">default</span><span style="color:#000;font-weight:bold">:</span>
</span></span><span style="display:flex;"><span>      <span style="color:#000;font-weight:bold">break</span>;
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#000;font-weight:bold">return</span> output;
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><h4 id="the-backward-pass">The backward pass</h4>
<p>For the training phase we need to define a loss function that we apply at the last layers output and then propagate the gradients backward through the network using the backward pass. This involves computing the gradients of the loss with respect to each weight and bias in the network and then updating these parameters using a gradient-based optimization method, such as stochastic gradient descent (SGD). The backward pass ensures that the network parameters are adjusted in a direction that minimizes the loss, thereby improving the model&rsquo;s performance over successive epochs and batches.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-c" data-lang="c"><span style="display:flex;"><span><span style="color:#458;font-weight:bold">void</span> <span style="color:#900;font-weight:bold">train_multilayer_perceptron</span>(multilayer_perceptron <span style="color:#000;font-weight:bold">*</span>mlp,
</span></span><span style="display:flex;"><span>                                 variable <span style="color:#000;font-weight:bold">**</span>x_batches, variable <span style="color:#000;font-weight:bold">**</span>y_batches,
</span></span><span style="display:flex;"><span>                                 <span style="color:#458;font-weight:bold">int</span> n_batches, <span style="color:#458;font-weight:bold">int</span> n_epochs,
</span></span><span style="display:flex;"><span>                                 NDARRAY_TYPE learning_rate,
</span></span><span style="display:flex;"><span>                                 variable <span style="color:#000;font-weight:bold">*</span>(<span style="color:#000;font-weight:bold">*</span>loss_fn)(variable <span style="color:#000;font-weight:bold">*</span>, variable <span style="color:#000;font-weight:bold">*</span>)) {
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  variable <span style="color:#000;font-weight:bold">**</span>weights <span style="color:#000;font-weight:bold">=</span> (variable <span style="color:#000;font-weight:bold">**</span>)<span style="color:#900;font-weight:bold">malloc</span>(mlp<span style="color:#000;font-weight:bold">-&gt;</span>n_layers <span style="color:#000;font-weight:bold">*</span> <span style="color:#000;font-weight:bold">sizeof</span>(variable <span style="color:#000;font-weight:bold">*</span>));
</span></span><span style="display:flex;"><span>  variable <span style="color:#000;font-weight:bold">**</span>bias <span style="color:#000;font-weight:bold">=</span> (variable <span style="color:#000;font-weight:bold">**</span>)<span style="color:#900;font-weight:bold">malloc</span>(mlp<span style="color:#000;font-weight:bold">-&gt;</span>n_layers <span style="color:#000;font-weight:bold">*</span> <span style="color:#000;font-weight:bold">sizeof</span>(variable <span style="color:#000;font-weight:bold">*</span>));
</span></span><span style="display:flex;"><span>  <span style="color:#000;font-weight:bold">for</span> (<span style="color:#458;font-weight:bold">int</span> i <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">0</span>; i <span style="color:#000;font-weight:bold">&lt;</span> n_epochs; i<span style="color:#000;font-weight:bold">++</span>) {
</span></span><span style="display:flex;"><span>    <span style="color:#000;font-weight:bold">for</span> (<span style="color:#458;font-weight:bold">int</span> j <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">0</span>; j <span style="color:#000;font-weight:bold">&lt;</span> n_batches; j<span style="color:#000;font-weight:bold">++</span>) {
</span></span><span style="display:flex;"><span>      variable <span style="color:#000;font-weight:bold">*</span>x_batch <span style="color:#000;font-weight:bold">=</span> <span style="color:#900;font-weight:bold">shallow_copy_variable</span>(x_batches[j]);
</span></span><span style="display:flex;"><span>      variable <span style="color:#000;font-weight:bold">*</span>y_batch <span style="color:#000;font-weight:bold">=</span> <span style="color:#900;font-weight:bold">shallow_copy_variable</span>(y_batches[j]);
</span></span><span style="display:flex;"><span>      variable <span style="color:#000;font-weight:bold">*</span>y_hat_batch <span style="color:#000;font-weight:bold">=</span> <span style="color:#900;font-weight:bold">forward_batch_multilayer_perceptron</span>(mlp, x_batch);
</span></span><span style="display:flex;"><span>      variable <span style="color:#000;font-weight:bold">*</span>loss_batch <span style="color:#000;font-weight:bold">=</span> <span style="color:#900;font-weight:bold">loss_fn</span>(y_hat_batch, y_batch);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>      <span style="color:#900;font-weight:bold">zero_grad_multilayer_perceptron</span>(mlp);
</span></span><span style="display:flex;"><span>      <span style="color:#900;font-weight:bold">backward_variable</span>(loss_batch);
</span></span><span style="display:flex;"><span>      <span style="color:#000;font-weight:bold">if</span> (j <span style="color:#000;font-weight:bold">%</span> <span style="color:#099">100</span> <span style="color:#000;font-weight:bold">==</span> <span style="color:#099">0</span>) {
</span></span><span style="display:flex;"><span>        <span style="color:#900;font-weight:bold">printf</span>(<span style="color:#d14">&#34;Epoch %d, batch %d, loss: %f</span><span style="color:#d14">\n</span><span style="color:#d14">&#34;</span>, i, j,
</span></span><span style="display:flex;"><span>               <span style="color:#900;font-weight:bold">sum_all_ndarray</span>(loss_batch<span style="color:#000;font-weight:bold">-&gt;</span>val));
</span></span><span style="display:flex;"><span>      }
</span></span><span style="display:flex;"><span>      <span style="color:#900;font-weight:bold">update_multilayer_perceptron</span>(mlp, learning_rate);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>      <span style="color:#000;font-weight:bold">for</span> (<span style="color:#458;font-weight:bold">int</span> k <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">0</span>; k <span style="color:#000;font-weight:bold">&lt;</span> mlp<span style="color:#000;font-weight:bold">-&gt;</span>n_layers; k<span style="color:#000;font-weight:bold">++</span>) {
</span></span><span style="display:flex;"><span>        weights[k] <span style="color:#000;font-weight:bold">=</span> <span style="color:#900;font-weight:bold">shallow_copy_variable</span>(mlp<span style="color:#000;font-weight:bold">-&gt;</span>weights[k]);
</span></span><span style="display:flex;"><span>        bias[k] <span style="color:#000;font-weight:bold">=</span> <span style="color:#900;font-weight:bold">shallow_copy_variable</span>(mlp<span style="color:#000;font-weight:bold">-&gt;</span>bias[k]);
</span></span><span style="display:flex;"><span>      }
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>      <span style="color:#900;font-weight:bold">free_graph_variable</span>(<span style="color:#000;font-weight:bold">&amp;</span>loss_batch);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>      <span style="color:#000;font-weight:bold">for</span> (<span style="color:#458;font-weight:bold">int</span> k <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">0</span>; k <span style="color:#000;font-weight:bold">&lt;</span> mlp<span style="color:#000;font-weight:bold">-&gt;</span>n_layers; k<span style="color:#000;font-weight:bold">++</span>) {
</span></span><span style="display:flex;"><span>        mlp<span style="color:#000;font-weight:bold">-&gt;</span>weights[k] <span style="color:#000;font-weight:bold">=</span> weights[k];
</span></span><span style="display:flex;"><span>        mlp<span style="color:#000;font-weight:bold">-&gt;</span>bias[k] <span style="color:#000;font-weight:bold">=</span> bias[k];
</span></span><span style="display:flex;"><span>      }
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>  <span style="color:#900;font-weight:bold">free</span>(weights);
</span></span><span style="display:flex;"><span>  <span style="color:#900;font-weight:bold">free</span>(bias);
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>The <code>update_multilayer_perceptron</code> function adjusts the weights and biases of each layer in the multilayer perceptron using the gradients computed during the backward pass. For each layer, it scales the gradients of the weights and biases by the learning rate.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-c" data-lang="c"><span style="display:flex;"><span><span style="color:#458;font-weight:bold">void</span> <span style="color:#900;font-weight:bold">update_multilayer_perceptron</span>(multilayer_perceptron <span style="color:#000;font-weight:bold">*</span>mlp,
</span></span><span style="display:flex;"><span>                                  NDARRAY_TYPE learning_rate) {
</span></span><span style="display:flex;"><span>  ndarray <span style="color:#000;font-weight:bold">*</span>place_holder;
</span></span><span style="display:flex;"><span>  ndarray <span style="color:#000;font-weight:bold">*</span>temp;
</span></span><span style="display:flex;"><span>  <span style="color:#000;font-weight:bold">for</span> (<span style="color:#458;font-weight:bold">int</span> i <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">0</span>; i <span style="color:#000;font-weight:bold">&lt;</span> mlp<span style="color:#000;font-weight:bold">-&gt;</span>n_layers; i<span style="color:#000;font-weight:bold">++</span>) {
</span></span><span style="display:flex;"><span>    place_holder <span style="color:#000;font-weight:bold">=</span> mlp<span style="color:#000;font-weight:bold">-&gt;</span>weights[i]<span style="color:#000;font-weight:bold">-&gt;</span>val;
</span></span><span style="display:flex;"><span>    temp <span style="color:#000;font-weight:bold">=</span> <span style="color:#900;font-weight:bold">multiply_ndarray_scalar</span>(mlp<span style="color:#000;font-weight:bold">-&gt;</span>weights[i]<span style="color:#000;font-weight:bold">-&gt;</span>grad, learning_rate);
</span></span><span style="display:flex;"><span>    mlp<span style="color:#000;font-weight:bold">-&gt;</span>weights[i]<span style="color:#000;font-weight:bold">-&gt;</span>val <span style="color:#000;font-weight:bold">=</span> <span style="color:#900;font-weight:bold">subtract_ndarray_ndarray</span>(mlp<span style="color:#000;font-weight:bold">-&gt;</span>weights[i]<span style="color:#000;font-weight:bold">-&gt;</span>val, temp);
</span></span><span style="display:flex;"><span>    <span style="color:#900;font-weight:bold">free_ndarray</span>(<span style="color:#000;font-weight:bold">&amp;</span>place_holder);
</span></span><span style="display:flex;"><span>    <span style="color:#900;font-weight:bold">free_ndarray</span>(<span style="color:#000;font-weight:bold">&amp;</span>temp);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    place_holder <span style="color:#000;font-weight:bold">=</span> mlp<span style="color:#000;font-weight:bold">-&gt;</span>bias[i]<span style="color:#000;font-weight:bold">-&gt;</span>val;
</span></span><span style="display:flex;"><span>    temp <span style="color:#000;font-weight:bold">=</span> <span style="color:#900;font-weight:bold">multiply_ndarray_scalar</span>(mlp<span style="color:#000;font-weight:bold">-&gt;</span>bias[i]<span style="color:#000;font-weight:bold">-&gt;</span>grad, learning_rate);
</span></span><span style="display:flex;"><span>    mlp<span style="color:#000;font-weight:bold">-&gt;</span>bias[i]<span style="color:#000;font-weight:bold">-&gt;</span>val <span style="color:#000;font-weight:bold">=</span> <span style="color:#900;font-weight:bold">subtract_ndarray_ndarray</span>(mlp<span style="color:#000;font-weight:bold">-&gt;</span>bias[i]<span style="color:#000;font-weight:bold">-&gt;</span>val, temp);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#900;font-weight:bold">free_ndarray</span>(<span style="color:#000;font-weight:bold">&amp;</span>place_holder);
</span></span><span style="display:flex;"><span>    <span style="color:#900;font-weight:bold">free_ndarray</span>(<span style="color:#000;font-weight:bold">&amp;</span>temp);
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>For more info check <a href="https://github.com/smdaa/teeny-autograd-c/blob/main/src/multilayer_perceptron.c">multilayer_perceptron.c</a></p>
<h2 id="tests">Tests</h2>
<p><code>ndarray</code>, <code>variable</code> and <code>multilayer_perceptron</code> structures were tested by comparing the outputs of each function against their counterparts in popular libraries like NumPy and PyTorch, using the libmocka-c framework for unit testing.</p>
<p>For instance, consider the test for the sigmoid operation:</p>
<p>This Python function generates test data for unary operations like the sigmoid function. It creates random input data x, computes the output y using the specified unary operation, and performs a backward pass with a random gradient z. The input, output, and gradients are saved to files for comparison.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#000;font-weight:bold">def</span> <span style="color:#900;font-weight:bold">generate_unary_op_test_data</span>(test, output_dir, unary_op, x_shape):
</span></span><span style="display:flex;"><span>    dir_path <span style="color:#000;font-weight:bold">=</span> os<span style="color:#000;font-weight:bold">.</span>path<span style="color:#000;font-weight:bold">.</span>join(output_dir, test)
</span></span><span style="display:flex;"><span>    <span style="color:#000;font-weight:bold">if</span> <span style="color:#000;font-weight:bold">not</span> os<span style="color:#000;font-weight:bold">.</span>path<span style="color:#000;font-weight:bold">.</span>exists(dir_path):
</span></span><span style="display:flex;"><span>        os<span style="color:#000;font-weight:bold">.</span>makedirs(dir_path)
</span></span><span style="display:flex;"><span>    x <span style="color:#000;font-weight:bold">=</span> torch<span style="color:#000;font-weight:bold">.</span>rand(x_shape, dtype<span style="color:#000;font-weight:bold">=</span>torch<span style="color:#000;font-weight:bold">.</span>double, requires_grad<span style="color:#000;font-weight:bold">=</span><span style="color:#000;font-weight:bold">True</span>)
</span></span><span style="display:flex;"><span>    y <span style="color:#000;font-weight:bold">=</span> unary_op(x)
</span></span><span style="display:flex;"><span>    z <span style="color:#000;font-weight:bold">=</span> torch<span style="color:#000;font-weight:bold">.</span>rand(x_shape, dtype<span style="color:#000;font-weight:bold">=</span>torch<span style="color:#000;font-weight:bold">.</span>double, requires_grad<span style="color:#000;font-weight:bold">=</span><span style="color:#000;font-weight:bold">True</span>)
</span></span><span style="display:flex;"><span>    y<span style="color:#000;font-weight:bold">.</span>backward(z)
</span></span><span style="display:flex;"><span>    save_ndarray_to_file(os<span style="color:#000;font-weight:bold">.</span>path<span style="color:#000;font-weight:bold">.</span>join(dir_path, <span style="color:#d14">&#34;x.txt&#34;</span>), x<span style="color:#000;font-weight:bold">.</span>detach()<span style="color:#000;font-weight:bold">.</span>numpy())
</span></span><span style="display:flex;"><span>    save_ndarray_to_file(os<span style="color:#000;font-weight:bold">.</span>path<span style="color:#000;font-weight:bold">.</span>join(dir_path, <span style="color:#d14">&#34;y.txt&#34;</span>), y<span style="color:#000;font-weight:bold">.</span>detach()<span style="color:#000;font-weight:bold">.</span>numpy())
</span></span><span style="display:flex;"><span>    save_ndarray_to_file(os<span style="color:#000;font-weight:bold">.</span>path<span style="color:#000;font-weight:bold">.</span>join(dir_path, <span style="color:#d14">&#34;z.txt&#34;</span>), z<span style="color:#000;font-weight:bold">.</span>detach()<span style="color:#000;font-weight:bold">.</span>numpy())
</span></span><span style="display:flex;"><span>    save_ndarray_to_file(os<span style="color:#000;font-weight:bold">.</span>path<span style="color:#000;font-weight:bold">.</span>join(dir_path, <span style="color:#d14">&#34;x_grad.txt&#34;</span>), x<span style="color:#000;font-weight:bold">.</span>grad<span style="color:#000;font-weight:bold">.</span>detach()<span style="color:#000;font-weight:bold">.</span>numpy())
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#998;font-style:italic"># test_sigmoid_variable</span>
</span></span><span style="display:flex;"><span>    n <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">10</span>
</span></span><span style="display:flex;"><span>    x_shape <span style="color:#000;font-weight:bold">=</span> (n, n)
</span></span><span style="display:flex;"><span>    generate_unary_op_test_data(
</span></span><span style="display:flex;"><span>        <span style="color:#d14">&#34;test_sigmoid_variable&#34;</span>, args<span style="color:#000;font-weight:bold">.</span>output_dir, torch<span style="color:#000;font-weight:bold">.</span>sigmoid, x_shape
</span></span><span style="display:flex;"><span>    )
</span></span></code></pre></div><p>In the corresponding C test:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-c" data-lang="c"><span style="display:flex;"><span><span style="color:#000;font-weight:bold">static</span> <span style="color:#458;font-weight:bold">void</span> <span style="color:#900;font-weight:bold">test_sigmoid_variable</span>(<span style="color:#458;font-weight:bold">void</span> <span style="color:#000;font-weight:bold">**</span>state) {
</span></span><span style="display:flex;"><span>  (<span style="color:#458;font-weight:bold">void</span>)state;
</span></span><span style="display:flex;"><span>  <span style="color:#458;font-weight:bold">char</span> path[<span style="color:#099">256</span>];
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#900;font-weight:bold">snprintf</span>(path, <span style="color:#000;font-weight:bold">sizeof</span>(path), <span style="color:#d14">&#34;%s/test_sigmoid_variable/x.txt&#34;</span>, dataDir);
</span></span><span style="display:flex;"><span>  ndarray <span style="color:#000;font-weight:bold">*</span>x <span style="color:#000;font-weight:bold">=</span> <span style="color:#900;font-weight:bold">read_ndarray</span>(path);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#900;font-weight:bold">snprintf</span>(path, <span style="color:#000;font-weight:bold">sizeof</span>(path), <span style="color:#d14">&#34;%s/test_sigmoid_variable/y.txt&#34;</span>, dataDir);
</span></span><span style="display:flex;"><span>  ndarray <span style="color:#000;font-weight:bold">*</span>y <span style="color:#000;font-weight:bold">=</span> <span style="color:#900;font-weight:bold">read_ndarray</span>(path);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#900;font-weight:bold">snprintf</span>(path, <span style="color:#000;font-weight:bold">sizeof</span>(path), <span style="color:#d14">&#34;%s/test_sigmoid_variable/z.txt&#34;</span>, dataDir);
</span></span><span style="display:flex;"><span>  ndarray <span style="color:#000;font-weight:bold">*</span>z <span style="color:#000;font-weight:bold">=</span> <span style="color:#900;font-weight:bold">read_ndarray</span>(path);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#900;font-weight:bold">snprintf</span>(path, <span style="color:#000;font-weight:bold">sizeof</span>(path), <span style="color:#d14">&#34;%s/test_sigmoid_variable/x_grad.txt&#34;</span>, dataDir);
</span></span><span style="display:flex;"><span>  ndarray <span style="color:#000;font-weight:bold">*</span>x_grad <span style="color:#000;font-weight:bold">=</span> <span style="color:#900;font-weight:bold">read_ndarray</span>(path);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  variable <span style="color:#000;font-weight:bold">*</span>var_x <span style="color:#000;font-weight:bold">=</span> <span style="color:#900;font-weight:bold">new_variable</span>(x);
</span></span><span style="display:flex;"><span>  variable <span style="color:#000;font-weight:bold">*</span>var_y_hat <span style="color:#000;font-weight:bold">=</span> <span style="color:#900;font-weight:bold">sigmoid_variable</span>(var_x);
</span></span><span style="display:flex;"><span>  <span style="color:#900;font-weight:bold">free_ndarray</span>(<span style="color:#000;font-weight:bold">&amp;</span>(var_y_hat<span style="color:#000;font-weight:bold">-&gt;</span>grad));
</span></span><span style="display:flex;"><span>  var_y_hat<span style="color:#000;font-weight:bold">-&gt;</span>grad <span style="color:#000;font-weight:bold">=</span> z;
</span></span><span style="display:flex;"><span>  <span style="color:#900;font-weight:bold">backward_variable</span>(var_y_hat);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#900;font-weight:bold">assert_true</span>(<span style="color:#900;font-weight:bold">is_equal_ndarray</span>(var_y_hat<span style="color:#000;font-weight:bold">-&gt;</span>val, y, NDARRAY_TYPE_EPSILON));
</span></span><span style="display:flex;"><span>  <span style="color:#900;font-weight:bold">assert_true</span>(<span style="color:#900;font-weight:bold">is_equal_ndarray</span>(var_x<span style="color:#000;font-weight:bold">-&gt;</span>grad, x_grad, NDARRAY_TYPE_EPSILON));
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#900;font-weight:bold">free_ndarray</span>(<span style="color:#000;font-weight:bold">&amp;</span>x);
</span></span><span style="display:flex;"><span>  <span style="color:#900;font-weight:bold">free_ndarray</span>(<span style="color:#000;font-weight:bold">&amp;</span>y);
</span></span><span style="display:flex;"><span>  <span style="color:#900;font-weight:bold">free_ndarray</span>(<span style="color:#000;font-weight:bold">&amp;</span>x_grad);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#900;font-weight:bold">free_graph_variable</span>(<span style="color:#000;font-weight:bold">&amp;</span>var_y_hat);
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>This C function reads the previously generated test data, performs the sigmoid operation on the ndarray encapsulated in a variable, and runs the backward pass. It then compares the resulting values and gradients to the expected outputs using assertions to ensure they are within a specified tolerance (<code>NDARRAY_TYPE_EPSILON</code>). This approach guarantees that the C implementation behaves correctly and consistently with the reference implementations in NumPy and PyTorch.</p>
<p>These unit tests were done for all the function, for more info check <a href="https://github.com/smdaa/teeny-autograd-c/tree/main/test">here</a></p>
<h2 id="examples">Examples</h2>
<h3 id="mnsit">MNSIT</h3>
<p>The MNIST dataset, consisting of 70,000 handwritten-digit images, is a classic benchmark for machine learning algorithms. It involves classifying digits from 0 to 9, providing an excellent test for our multilayer perceptron implementation. The following example demonstrates the setup, training, and evaluation of the model on the MNIST dataset.</p>
<p>Our model will be a 4 layer neural network with the following dimensions and activation function:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-c" data-lang="c"><span style="display:flex;"><span><span style="color:#458;font-weight:bold">int</span> n_layers <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">4</span>;
</span></span><span style="display:flex;"><span><span style="color:#458;font-weight:bold">int</span> in_sizes[] <span style="color:#000;font-weight:bold">=</span> {<span style="color:#099">28</span> <span style="color:#000;font-weight:bold">*</span> <span style="color:#099">28</span>, <span style="color:#099">64</span>, <span style="color:#099">32</span>, <span style="color:#099">16</span>};
</span></span><span style="display:flex;"><span><span style="color:#458;font-weight:bold">int</span> out_sizes[] <span style="color:#000;font-weight:bold">=</span> {<span style="color:#099">64</span>, <span style="color:#099">32</span>, <span style="color:#099">16</span>, <span style="color:#099">10</span>};
</span></span><span style="display:flex;"><span>activation_function activations[] <span style="color:#000;font-weight:bold">=</span> {SIGMOID, SIGMOID, SIGMOID, LINEAR};
</span></span></code></pre></div><p>We will train our model with the following parameters:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-c" data-lang="c"><span style="display:flex;"><span><span style="color:#458;font-weight:bold">int</span> batch_size <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">64</span>;
</span></span><span style="display:flex;"><span><span style="color:#458;font-weight:bold">int</span> n_epochs <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">100</span>;
</span></span><span style="display:flex;"><span>NDARRAY_TYPE learning_rate <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">0.01</span>;
</span></span></code></pre></div><p>and since this is a classification problem we need to define the loss function which is the cross entropy loss</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-c" data-lang="c"><span style="display:flex;"><span>variable <span style="color:#000;font-weight:bold">*</span><span style="color:#900;font-weight:bold">cross_entropy_loss</span>(variable <span style="color:#000;font-weight:bold">*</span>logits, variable <span style="color:#000;font-weight:bold">*</span>y) {
</span></span><span style="display:flex;"><span>  variable <span style="color:#000;font-weight:bold">*</span>y_hat_exp <span style="color:#000;font-weight:bold">=</span> <span style="color:#900;font-weight:bold">exp_variable</span>(logits);
</span></span><span style="display:flex;"><span>  variable <span style="color:#000;font-weight:bold">*</span>y_hat_sum <span style="color:#000;font-weight:bold">=</span> <span style="color:#900;font-weight:bold">sum_variable</span>(y_hat_exp, <span style="color:#099">1</span>);
</span></span><span style="display:flex;"><span>  ndarray <span style="color:#000;font-weight:bold">*</span>temp <span style="color:#000;font-weight:bold">=</span> <span style="color:#900;font-weight:bold">full_ndarray</span>(y_hat_sum<span style="color:#000;font-weight:bold">-&gt;</span>val<span style="color:#000;font-weight:bold">-&gt;</span>dim, y_hat_sum<span style="color:#000;font-weight:bold">-&gt;</span>val<span style="color:#000;font-weight:bold">-&gt;</span>shape,
</span></span><span style="display:flex;"><span>                               NDARRAY_TYPE_EPSILON);
</span></span><span style="display:flex;"><span>  variable <span style="color:#000;font-weight:bold">*</span>y_hat_log_sum <span style="color:#000;font-weight:bold">=</span>
</span></span><span style="display:flex;"><span>      <span style="color:#900;font-weight:bold">log_variable</span>(<span style="color:#900;font-weight:bold">add_variable</span>(y_hat_sum, <span style="color:#900;font-weight:bold">new_variable</span>(temp)));
</span></span><span style="display:flex;"><span>  <span style="color:#900;font-weight:bold">free_ndarray</span>(<span style="color:#000;font-weight:bold">&amp;</span>temp);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  variable <span style="color:#000;font-weight:bold">*</span>y_hat_softmax <span style="color:#000;font-weight:bold">=</span> <span style="color:#900;font-weight:bold">subtract_variable</span>(logits, y_hat_log_sum);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  variable <span style="color:#000;font-weight:bold">*</span>product <span style="color:#000;font-weight:bold">=</span> <span style="color:#900;font-weight:bold">multiply_variable</span>(y, y_hat_softmax);
</span></span><span style="display:flex;"><span>  variable <span style="color:#000;font-weight:bold">*</span>neg_product <span style="color:#000;font-weight:bold">=</span> <span style="color:#900;font-weight:bold">negate_variable</span>(product);
</span></span><span style="display:flex;"><span>  variable <span style="color:#000;font-weight:bold">*</span>loss <span style="color:#000;font-weight:bold">=</span> <span style="color:#900;font-weight:bold">sum_variable</span>(neg_product, <span style="color:#099">1</span>);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#900;font-weight:bold">free_ndarray</span>(<span style="color:#000;font-weight:bold">&amp;</span>(loss<span style="color:#000;font-weight:bold">-&gt;</span>grad));
</span></span><span style="display:flex;"><span>  loss<span style="color:#000;font-weight:bold">-&gt;</span>grad <span style="color:#000;font-weight:bold">=</span> <span style="color:#900;font-weight:bold">ones_ndarray</span>(loss<span style="color:#000;font-weight:bold">-&gt;</span>val<span style="color:#000;font-weight:bold">-&gt;</span>dim, loss<span style="color:#000;font-weight:bold">-&gt;</span>val<span style="color:#000;font-weight:bold">-&gt;</span>shape);
</span></span><span style="display:flex;"><span>  <span style="color:#000;font-weight:bold">return</span> loss;
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>If we start the training we can see that indeed our model is learning:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>Epoch 0, batch 0, loss: 153.069234
</span></span><span style="display:flex;"><span>Epoch 0, batch 100, loss: 149.035196
</span></span><span style="display:flex;"><span>Epoch 0, batch 200, loss: 146.707832
</span></span><span style="display:flex;"><span>Epoch 0, batch 300, loss: 118.086699
</span></span><span style="display:flex;"><span>Epoch 0, batch 400, loss: 86.300584
</span></span><span style="display:flex;"><span>Epoch 0, batch 500, loss: 66.593495
</span></span><span style="display:flex;"><span>Epoch 0, batch 600, loss: 59.607119
</span></span><span style="display:flex;"><span>Epoch 1, batch 0, loss: 57.373467
</span></span><span style="display:flex;"><span>Epoch 1, batch 100, loss: 53.599278
</span></span><span style="display:flex;"><span>Epoch 1, batch 200, loss: 33.338024
</span></span><span style="display:flex;"><span>Epoch 1, batch 300, loss: 29.048711
</span></span><span style="display:flex;"><span>Epoch 1, batch 400, loss: 38.811830
</span></span><span style="display:flex;"><span>Epoch 1, batch 500, loss: 24.286488
</span></span><span style="display:flex;"><span>Epoch 1, batch 600, loss: 22.726048
</span></span><span style="display:flex;"><span>Epoch 2, batch 0, loss: 28.801350
</span></span><span style="display:flex;"><span>Epoch 2, batch 100, loss: 35.327724
</span></span><span style="display:flex;"><span>Epoch 2, batch 200, loss: 14.522009
</span></span><span style="display:flex;"><span>Epoch 2, batch 300, loss: 14.909454
</span></span><span style="display:flex;"><span>Epoch 2, batch 400, loss: 17.792573
</span></span><span style="display:flex;"><span>Epoch 2, batch 500, loss: 6.376877
</span></span><span style="display:flex;"><span>Epoch 2, batch 600, loss: 13.702117
</span></span><span style="display:flex;"><span>Epoch 3, batch 0, loss: 19.182989
</span></span><span style="display:flex;"><span>Epoch 3, batch 100, loss: 28.739773
</span></span><span style="display:flex;"><span>Epoch 3, batch 200, loss: 11.350802
</span></span><span style="display:flex;"><span>Epoch 3, batch 300, loss: 13.056788
</span></span><span style="display:flex;"><span>Epoch 3, batch 400, loss: 11.537156
</span></span><span style="display:flex;"><span>Epoch 3, batch 500, loss: 4.421002
</span></span><span style="display:flex;"><span>Epoch 3, batch 600, loss: 10.465591
</span></span><span style="display:flex;"><span>Epoch 4, batch 0, loss: 15.420338
</span></span><span style="display:flex;"><span>Epoch 4, batch 100, loss: 23.214068
</span></span><span style="display:flex;"><span>Epoch 4, batch 200, loss: 10.336427
</span></span><span style="display:flex;"><span>Epoch 4, batch 300, loss: 10.720565
</span></span><span style="display:flex;"><span>Epoch 4, batch 400, loss: 8.834916
</span></span><span style="display:flex;"><span>Epoch 4, batch 500, loss: 3.295116
</span></span></code></pre></div><p>You can check the full example <a href="https://github.com/smdaa/teeny-autograd-c/blob/main/examples/mnist_mlp/mnist_mlp.c">here</a></p>
<h3 id="paint">Paint</h3>
<p>This is more of a fun example, where we only use the forward pass to create a sort of neural network shader, i.e., we input a vector that represents the coordinates of a pixel in the image, and we get back a vector that represents the color.</p>
<p>I landed on the following model:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-c" data-lang="c"><span style="display:flex;"><span><span style="color:#458;font-weight:bold">int</span> layer_size <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">32</span>;
</span></span><span style="display:flex;"><span>multilayer_perceptron <span style="color:#000;font-weight:bold">*</span>mlp <span style="color:#000;font-weight:bold">=</span> <span style="color:#900;font-weight:bold">new_multilayer_perceptron</span>(
</span></span><span style="display:flex;"><span>      <span style="color:#099">9</span>, batch_size,
</span></span><span style="display:flex;"><span>      (<span style="color:#458;font-weight:bold">int</span>[]){<span style="color:#099">3</span>, layer_size, layer_size, layer_size, layer_size, layer_size,
</span></span><span style="display:flex;"><span>              layer_size, layer_size, layer_size},
</span></span><span style="display:flex;"><span>      (<span style="color:#458;font-weight:bold">int</span>[]){layer_size, layer_size, layer_size, layer_size, layer_size,
</span></span><span style="display:flex;"><span>              layer_size, layer_size, layer_size, <span style="color:#099">3</span>},
</span></span><span style="display:flex;"><span>      (activation_function[]){TANH, TANH, TANH, TANH, TANH, TANH, TANH, TANH,
</span></span><span style="display:flex;"><span>                              SIGMOID},
</span></span><span style="display:flex;"><span>      (random_initialisation[]){NORMAL, NORMAL, NORMAL, NORMAL, NORMAL, NORMAL,
</span></span><span style="display:flex;"><span>                                NORMAL, NORMAL, NORMAL});
</span></span></code></pre></div><p>Using the following coordinates as an input:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-c" data-lang="c"><span style="display:flex;"><span>ndarray <span style="color:#000;font-weight:bold">*</span>x <span style="color:#000;font-weight:bold">=</span> <span style="color:#900;font-weight:bold">zeros_ndarray</span>(<span style="color:#099">2</span>, (<span style="color:#458;font-weight:bold">int</span>[]){height <span style="color:#000;font-weight:bold">*</span> width, <span style="color:#099">3</span>});
</span></span><span style="display:flex;"><span><span style="color:#000;font-weight:bold">for</span> (<span style="color:#458;font-weight:bold">int</span> i <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">0</span>; i <span style="color:#000;font-weight:bold">&lt;</span> height; i<span style="color:#000;font-weight:bold">++</span>) {
</span></span><span style="display:flex;"><span>  <span style="color:#000;font-weight:bold">for</span> (<span style="color:#458;font-weight:bold">int</span> j <span style="color:#000;font-weight:bold">=</span> <span style="color:#099">0</span>; j <span style="color:#000;font-weight:bold">&lt;</span> width; j<span style="color:#000;font-weight:bold">++</span>) {
</span></span><span style="display:flex;"><span>    x<span style="color:#000;font-weight:bold">-&gt;</span>data[<span style="color:#099">3</span> <span style="color:#000;font-weight:bold">*</span> (i <span style="color:#000;font-weight:bold">*</span> width <span style="color:#000;font-weight:bold">+</span> j)] <span style="color:#000;font-weight:bold">=</span> ((NDARRAY_TYPE)i <span style="color:#000;font-weight:bold">/</span> height <span style="color:#000;font-weight:bold">-</span> <span style="color:#099">0.5</span>) <span style="color:#000;font-weight:bold">*</span> zoom;
</span></span><span style="display:flex;"><span>    x<span style="color:#000;font-weight:bold">-&gt;</span>data[<span style="color:#099">3</span> <span style="color:#000;font-weight:bold">*</span> (i <span style="color:#000;font-weight:bold">*</span> width <span style="color:#000;font-weight:bold">+</span> j) <span style="color:#000;font-weight:bold">+</span> <span style="color:#099">1</span>] <span style="color:#000;font-weight:bold">=</span> ((NDARRAY_TYPE)j <span style="color:#000;font-weight:bold">/</span> width <span style="color:#000;font-weight:bold">-</span> <span style="color:#099">0.5</span>) <span style="color:#000;font-weight:bold">*</span> zoom;
</span></span><span style="display:flex;"><span>    x<span style="color:#000;font-weight:bold">-&gt;</span>data[<span style="color:#099">3</span> <span style="color:#000;font-weight:bold">*</span> (i <span style="color:#000;font-weight:bold">*</span> width <span style="color:#000;font-weight:bold">+</span> j) <span style="color:#000;font-weight:bold">+</span> <span style="color:#099">2</span>] <span style="color:#000;font-weight:bold">=</span>
</span></span><span style="display:flex;"><span>        <span style="color:#900;font-weight:bold">sqrt</span>(<span style="color:#900;font-weight:bold">pow</span>((NDARRAY_TYPE)i <span style="color:#000;font-weight:bold">/</span> height <span style="color:#000;font-weight:bold">-</span> <span style="color:#099">0.5</span>, <span style="color:#099">2.0</span>) <span style="color:#000;font-weight:bold">+</span>
</span></span><span style="display:flex;"><span>             <span style="color:#900;font-weight:bold">pow</span>((NDARRAY_TYPE)j <span style="color:#000;font-weight:bold">/</span> width <span style="color:#000;font-weight:bold">-</span> <span style="color:#099">0.5</span>, <span style="color:#099">2.0</span>)) <span style="color:#000;font-weight:bold">*</span>
</span></span><span style="display:flex;"><span>        zoom;
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>Here is an example of what you can get:</p>
<p><img src="/assets/building-an-autograd-library-from-scratch-in-c-for-simple-neural-networks/output1.png" alt="">
<img src="/assets/building-an-autograd-library-from-scratch-in-c-for-simple-neural-networks/output2.png" alt="">
<img src="/assets/building-an-autograd-library-from-scratch-in-c-for-simple-neural-networks/output3.png" alt=""></p>
<p>You can check the full example <a href="https://github.com/smdaa/teeny-autograd-c/blob/main/examples/paint/paint.c">here</a></p>
<h2 id="conclusion">Conclusion</h2>
<p>This Autograd library in C demonstrates essential neural network operations and automatic differentiation through a simplified, custom implementation. The code includes key components like tensor operations, neural network layers, and forward and backward propagation, all aimed at providing a clear educational example. Note that this implementation is primarily for instructional purposes, as real-world libraries are more complex and optimized for performance.</p>

</main>

  <footer>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body);"></script>
<script>
document.addEventListener("DOMContentLoaded", function() {
    renderMathInElement(document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "$", right: "$", display: false}
        ]
    });
});
</script>
  
  <hr/>
  © 2025 Saad Mdaa
  
  </footer>
  </body>
</html>

