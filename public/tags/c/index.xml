<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>C on smdaa</title>
    <link>http://localhost:1313/tags/c/</link>
    <description>Recent content in C on smdaa</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 29 Jul 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/c/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Building an autograd library from scratch in C for simple neural networks</title>
      <link>http://localhost:1313/post/building-an-autograd-library-from-scratch-in-c-for-simple-neural-networks/</link>
      <pubDate>Mon, 29 Jul 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/building-an-autograd-library-from-scratch-in-c-for-simple-neural-networks/</guid>
      <description>&lt;div class=&#34;toc&#34;&gt;&#xA;  &lt;h2&gt;Table of Contents&lt;/h2&gt;&#xA;  &lt;nav id=&#34;TableOfContents&#34;&gt;&#xA;  &lt;ul&gt;&#xA;    &lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;&#xA;    &lt;li&gt;&lt;a href=&#34;#neural-networks-a-brief-overview&#34;&gt;Neural networks: a brief overview&lt;/a&gt;&lt;/li&gt;&#xA;    &lt;li&gt;&lt;a href=&#34;#derivative-calculation-symbolic-numerical-and-automatic-differentiation&#34;&gt;Derivative calculation: symbolic, numerical and automatic differentiation&lt;/a&gt;&lt;/li&gt;&#xA;    &lt;li&gt;&lt;a href=&#34;#implementation&#34;&gt;Implementation&lt;/a&gt;&#xA;      &lt;ul&gt;&#xA;        &lt;li&gt;&lt;a href=&#34;#n-dimensional-array&#34;&gt;N-dimensional array&lt;/a&gt;&lt;/li&gt;&#xA;        &lt;li&gt;&lt;a href=&#34;#variable-node&#34;&gt;Variable node&lt;/a&gt;&lt;/li&gt;&#xA;        &lt;li&gt;&lt;a href=&#34;#multilayer-perceptron&#34;&gt;Multilayer perceptron&lt;/a&gt;&lt;/li&gt;&#xA;      &lt;/ul&gt;&#xA;    &lt;/li&gt;&#xA;    &lt;li&gt;&lt;a href=&#34;#tests&#34;&gt;Tests&lt;/a&gt;&lt;/li&gt;&#xA;    &lt;li&gt;&lt;a href=&#34;#examples&#34;&gt;Examples&lt;/a&gt;&#xA;      &lt;ul&gt;&#xA;        &lt;li&gt;&lt;a href=&#34;#mnsit&#34;&gt;MNSIT&lt;/a&gt;&lt;/li&gt;&#xA;        &lt;li&gt;&lt;a href=&#34;#paint&#34;&gt;Paint&lt;/a&gt;&lt;/li&gt;&#xA;      &lt;/ul&gt;&#xA;    &lt;/li&gt;&#xA;    &lt;li&gt;&lt;a href=&#34;#conclusion&#34;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;&#xA;  &lt;/ul&gt;&#xA;&lt;/nav&gt;&#xA;&lt;/div&gt;&#xA;&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;Autograd is a fundamental component in machine learning frameworks, enabling the automatic computation of gradients for training neural networks. This article will walk you through my journey of writing an Autograd library from scratch (no third-party libraries) in pure C.&lt;/p&gt;&#xA;&lt;p&gt;&lt;a href=&#34;https://github.com/smdaa/teeny-autograd-c&#34;&gt;View source code on GitHub&lt;/a&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Optimizing CPU matrix multiplication</title>
      <link>http://localhost:1313/post/optimizing-cpu-matrix-multiplication/</link>
      <pubDate>Sun, 19 May 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/optimizing-cpu-matrix-multiplication/</guid>
      <description>&lt;div class=&#34;toc&#34;&gt;&#xA;  &lt;h2&gt;Table of Contents&lt;/h2&gt;&#xA;  &lt;nav id=&#34;TableOfContents&#34;&gt;&#xA;  &lt;ul&gt;&#xA;    &lt;li&gt;&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;&#xA;    &lt;li&gt;&lt;a href=&#34;#blasopenblas&#34;&gt;BLAS/OpenBLAS&lt;/a&gt;&lt;/li&gt;&#xA;    &lt;li&gt;&lt;a href=&#34;#testing-environment&#34;&gt;Testing environment&lt;/a&gt;&lt;/li&gt;&#xA;    &lt;li&gt;&lt;a href=&#34;#naive-gemm&#34;&gt;Naive gemm&lt;/a&gt;&lt;/li&gt;&#xA;    &lt;li&gt;&lt;a href=&#34;#naive-gemm-improvements&#34;&gt;Naive gemm improvements&lt;/a&gt;&#xA;      &lt;ul&gt;&#xA;        &lt;li&gt;&lt;a href=&#34;#loop-interchange&#34;&gt;Loop interchange&lt;/a&gt;&lt;/li&gt;&#xA;        &lt;li&gt;&lt;a href=&#34;#single-instruction-multiple-data&#34;&gt;Single Instruction Multiple Data&lt;/a&gt;&lt;/li&gt;&#xA;        &lt;li&gt;&lt;a href=&#34;#openmp-parallelization&#34;&gt;OpenMP parallelization&lt;/a&gt;&lt;/li&gt;&#xA;      &lt;/ul&gt;&#xA;    &lt;/li&gt;&#xA;    &lt;li&gt;&lt;a href=&#34;#conclusion&#34;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;&#xA;  &lt;/ul&gt;&#xA;&lt;/nav&gt;&#xA;&lt;/div&gt;&#xA;&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;&#xA;&lt;p&gt;Matrix multiplication, denoted as &lt;strong&gt;gemm&lt;/strong&gt; (general matrix multiplication), is a fundamental operation in linear algebra and forms the backbone of numerous scientific computing and machine learning applications.&lt;/p&gt;&#xA;&lt;p&gt;Gemm computes the product of two matrices, with the resulting matrix being the linear combination of the rows of the first matrix and the columns of the second matrix. While conceptually simple, this operation is computationally intensive and often can be a performance bottleneck in many algorithms.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
